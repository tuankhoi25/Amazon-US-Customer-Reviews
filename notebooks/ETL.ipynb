{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b923b5-a59a-46c5-ae73-3f3cfc74cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5c527-ba8c-4b19-8798-6e35ace759bf",
   "metadata": {},
   "source": [
    "# Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb421474-43ec-4905-a941-d5523fc637a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pyspark\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timezone, timedelta, date\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col \n",
    "from pyspark.sql.functions import max\n",
    "from typing import List\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f5005b-9c8f-4105-a6c7-2d7fb039301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_ENDPOINT = os.getenv(\"AWS_S3_ENDPOINT\")\n",
    "NESSIE_URI = os.getenv(\"NESSIE_URI\")\n",
    "WAREHOUSE = os.getenv(\"WAREHOUSE\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d768eb0-7503-4576-a365-e88dbcd7c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ff1f7cf0-6815-4ddb-9213-241315657b8c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.104.3 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.9.2 in central\n",
      "\tfound org.postgresql#postgresql;42.5.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.9.2/iceberg-spark-runtime-3.5_2.12-1.9.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2!iceberg-spark-runtime-3.5_2.12.jar (9159ms)\n",
      "downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.104.3/nessie-spark-extensions-3.5_2.12-0.104.3.jar ...\n",
      "\t[SUCCESSFUL ] org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.104.3!nessie-spark-extensions-3.5_2.12.jar (915ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-aws-bundle/1.9.2/iceberg-aws-bundle-1.9.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-aws-bundle;1.9.2!iceberg-aws-bundle.jar (11908ms)\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.5.0!postgresql.jar (830ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (578ms)\n",
      ":: resolution report :: resolve 5662ms :: artifacts dl 23406ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.9.2 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.104.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ff1f7cf0-6815-4ddb-9213-241315657b8c\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (106786kB/206ms)\n",
      "25/09/29 04:58:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', \",\".join([\n",
    "            'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.2',\n",
    "            'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.104.3',\n",
    "            'org.apache.iceberg:iceberg-aws-bundle:1.9.2',\n",
    "            'org.postgresql:postgresql:42.5.0'\n",
    "            ]))\n",
    "        .set('spark.sql.extensions', \",\".join([\n",
    "            'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions',\n",
    "            'org.projectnessie.spark.extensions.NessieSparkSessionExtensions'\n",
    "        ]))\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', NESSIE_URI)\n",
    "        .set('spark.sql.catalog.nessie.client-api-version', '2')\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', AWS_S3_ENDPOINT)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.s3.path-style-access', 'true')\n",
    "        .set('spark.sql.catalog.nessie.s3.access-key-id', AWS_ACCESS_KEY_ID)\n",
    "        .set('spark.sql.catalog.nessie.s3.secret-access-key', AWS_SECRET_ACCESS_KEY)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set(\"spark.sql.iceberg.merge-schema\", \"true\")\n",
    "        .set(\"spark.executor.memory\", \"2g\")\n",
    "        .set(\"spark.driver.memory\", \"2g\")\n",
    "        .set(\"spark.executor.cores\", \"2\")\n",
    ")\n",
    "\n",
    "# Start Spark Session\n",
    "spark = SparkSession.builder.master(\"spark://spark-master:7077\").config(conf=conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea258d-9d8d-41c6-9e1b-70f66a6c9c6c",
   "metadata": {},
   "source": [
    "# DDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cd6dde-687f-4f96-a7af-76d29ea14b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(\"CREATE DATABASE nessie.bronze\")\n",
    "spark.sql(\"CREATE DATABASE nessie.silver\")\n",
    "spark.sql(\"CREATE DATABASE nessie.gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cfc5e-77b8-46ff-a117-971082030c31",
   "metadata": {},
   "source": [
    "## Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdca3d4f-c275-4dc0-8a94-2cd4969c06bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.customer (\n",
    "        id BIGINT,\n",
    "        name STRING,\n",
    "        sex STRING,\n",
    "        mail STRING,\n",
    "        birthdate DATE,\n",
    "        login_username STRING,\n",
    "        login_password STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.location (\n",
    "        id STRING,\n",
    "        street_address STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        zipcode INT,\n",
    "        country STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.customer_location (\n",
    "        id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        location_id STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.phone_number (\n",
    "        id STRING,\n",
    "        phone_number STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.customer_phone (\n",
    "        id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        phone_id STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.shadow_product (\n",
    "        id STRING,\n",
    "        product_id STRING,\n",
    "        product_title STRING,\n",
    "        currency STRING,\n",
    "        price DECIMAL(10, 2),\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.category (\n",
    "        id BIGINT,\n",
    "        category_name STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.product_category (\n",
    "        id BIGINT,\n",
    "        product_id STRING,\n",
    "        category_id BIGINT,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.bronze.review (\n",
    "        id STRING,\n",
    "        customer_id BIGINT,\n",
    "        product_id STRING,\n",
    "        star_rating STRING,\n",
    "        helpful_votes INT,\n",
    "        total_votes INT,\n",
    "        marketplace STRING,\n",
    "        verified_purchase STRING,\n",
    "        review_headline STRING,\n",
    "        review_body STRING,\n",
    "        created_at DATE,\n",
    "        updated_at DATE,\n",
    "        _ingested_at TIMESTAMP,\n",
    "        _batch_id STRING,\n",
    "        _is_deleted BOOLEAN\n",
    "    )\n",
    "    USING iceberg\n",
    "    TBLPROPERTIES ('write.spark.accept-any-schema'='true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d8974-a704-4f59-91f1-251ef8f8112f",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1244923d-c72d-4d1f-a0c1-aa590027f279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.customer (\n",
    "        customer_id BIGINT,\n",
    "        name STRING,\n",
    "        sex STRING,\n",
    "        mail STRING,\n",
    "        birthdate DATE,\n",
    "        signup_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.location (\n",
    "        location_id STRING,\n",
    "        street_address STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        zipcode STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.customer_location (\n",
    "        customer_location_id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        location_id STRING,\n",
    "        source_created_at DATE,\n",
    "        source_updated_at DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.product (\n",
    "        product_id STRING,\n",
    "        product_title STRING,\n",
    "        currency STRING,\n",
    "        price DECIMAL(10, 2),\n",
    "        source_created_at DATE,\n",
    "        source_updated_at DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.category (\n",
    "        category_id BIGINT,\n",
    "        category_name STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.product_category (\n",
    "        product_category_id BIGINT,\n",
    "        product_id STRING,\n",
    "        category_id BIGINT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.silver.review (\n",
    "        review_id STRING,\n",
    "        customer_id BIGINT,\n",
    "        product_id STRING,\n",
    "        star_rating INT,\n",
    "        helpful_votes INT,\n",
    "        total_votes INT,\n",
    "        marketplace STRING,\n",
    "        verified_purchase BOOLEAN,\n",
    "        review_headline STRING,\n",
    "        review_body STRING,\n",
    "        modified_date DATE\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bfb614-019c-462b-9bf5-b44ed7a45d40",
   "metadata": {},
   "source": [
    "## Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16130895-716b-4bc5-94d0-677feba12650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.dim_customer (\n",
    "        customer_key BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        name STRING,\n",
    "        sex STRING,\n",
    "        mail STRING,\n",
    "        birthdate DATE,\n",
    "        signup_date DATE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.dim_date (\n",
    "        date_key BIGINT,\n",
    "        full_date DATE,\n",
    "        day INT,\n",
    "        month INT,\n",
    "        month_name STRING,\n",
    "        quarter INT,\n",
    "        quarter_name STRING,\n",
    "        year INT,\n",
    "        day_of_week INT,\n",
    "        day_name STRING,\n",
    "        week_of_year INT,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.dim_location (\n",
    "        location_key BIGINT,\n",
    "        location_id STRING,\n",
    "        street_address STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        zipcode STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.bridge_customer_location (\n",
    "        customer_location_key BIGINT,\n",
    "        customer_location_id BIGINT,\n",
    "        location_key INT,\n",
    "        customer_key INT,\n",
    "        valid_from DATE,\n",
    "        valid_to DATE,\n",
    "        is_current BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.dim_product (\n",
    "        product_key BIGINT,\n",
    "        product_id STRING,\n",
    "        product_title STRING,\n",
    "        currency STRING,\n",
    "        price DECIMAL(10, 2),\n",
    "        valid_from DATE,\n",
    "        valid_to DATE,\n",
    "        is_current BOOLEAN\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.dim_category (\n",
    "        category_key BIGINT,\n",
    "        category_id BIGINT,\n",
    "        category_name STRING\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.bridge_product_category (\n",
    "        product_category_key BIGINT,\n",
    "        product_category_id BIGINT,\n",
    "        product_key INT,\n",
    "        category_key INT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE nessie.gold.fact_review (\n",
    "        review_key BIGINT,\n",
    "        review_id STRING,\n",
    "        product_key INT,\n",
    "        customer_key INT,\n",
    "        date_key INT,\n",
    "        star_rating INT,\n",
    "        helpful_votes INT,\n",
    "        total_votes INT,\n",
    "        marketplace STRING,\n",
    "        verified_purchase BOOLEAN,\n",
    "        review_headline STRING,\n",
    "        review_body STRING,\n",
    "        is_current BOOLEAN\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73620f-4891-49e3-9120-e09758575976",
   "metadata": {},
   "source": [
    "# ETL processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3719543-92ac-4a9a-b87b-5a5c02ecb16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[refType: string, name: string, hash: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = datetime.now(timezone(timedelta(hours=7))).replace(tzinfo=None).replace(microsecond=0)\n",
    "ts = datetime(2012, 1, 1, 1, 1, 1)\n",
    "\n",
    "today = str(ts.date())\n",
    "\n",
    "etl_processing_branch_name = f\"feat/etl-processing-{ts.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE BRANCH\n",
    "    IF NOT EXISTS {etl_processing_branch_name}\n",
    "    IN nessie\n",
    "    FROM main\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e8bb3-9a84-4e69-8779-5f570f56d71c",
   "metadata": {},
   "source": [
    "## Bronze Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d9e70-c6b6-4ae3-8a70-339df0fe0413",
   "metadata": {},
   "source": [
    "Create new branch for etl:\n",
    "\n",
    "    - If success, merge branch\n",
    "    - If fail, nothing change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63644e5c-f72c-476f-8ebe-c6570021295c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------+----------------------------------------------------------------+\n",
      "|refType|name                                   |hash                                                            |\n",
      "+-------+---------------------------------------+----------------------------------------------------------------+\n",
      "|Branch |feat/bronze-layer-2012-01-01-01-01-01  |2316eefd35c51de5b350867775a59b513a423f7a5531e9244403ed652f058633|\n",
      "|Branch |feat/etl-processing-2012-01-01-01-01-01|2316eefd35c51de5b350867775a59b513a423f7a5531e9244403ed652f058633|\n",
      "|Branch |main                                   |2316eefd35c51de5b350867775a59b513a423f7a5531e9244403ed652f058633|\n",
      "+-------+---------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bronze_layer_branch_name = f\"feat/bronze-layer-{ts.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE BRANCH\n",
    "    IF NOT EXISTS {bronze_layer_branch_name}\n",
    "    IN nessie\n",
    "    FROM {etl_processing_branch_name}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"USE REFERENCE {bronze_layer_branch_name} IN nessie;\")\n",
    "spark.sql(\"LIST REFERENCES IN nessie\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a282e7a6-d950-4a10-b72f-d7e41866d1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read(table_name: str, today: date):\n",
    "    # Lấy thời gian gần nhất mà bronze layer được update\n",
    "    max_updated_at = spark.sql(f\"SELECT max(updated_at) as max_updated_at FROM nessie.bronze.{table_name}\").collect()[0][\"max_updated_at\"]\n",
    "    max_updated_at = \"1999-01-01\" if max_updated_at is None else str(max_updated_at)\n",
    "    print(f\"max_updated_at: {max_updated_at}\")\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    (\n",
    "        SELECT \n",
    "            min(updated_at) AS min_date, \n",
    "            max(updated_at) AS max_date \n",
    "        FROM {table_name} \n",
    "        WHERE updated_at > '{max_updated_at}' AND updated_at <= '{today}'\n",
    "    ) tmp\n",
    "    \"\"\"\n",
    "    bounds = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/oltp\") \\\n",
    "        .option(\"dbtable\", query) \\\n",
    "        .option(\"user\", \"postgres\") \\\n",
    "        .option(\"password\", \"postgres\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .load() \\\n",
    "        .collect()[0]\n",
    "    \n",
    "    lower, upper = bounds[\"min_date\"], bounds[\"max_date\"]\n",
    "    if lower is None or upper is None:\n",
    "        print(f\"Không có bản ghi mới cho {table_name}, skip ingest.\")\n",
    "        df_empty = spark.createDataFrame([], StructType([]))\n",
    "        return df_empty\n",
    "    print(f\"lower: {lower}, upper: {upper}\")\n",
    "    \n",
    "    # Chỉ lấy những record ở source có giá trị updated_at > giá trị max(updated_at) ở bronze\n",
    "    query = f\"\"\"\n",
    "    (\n",
    "        SELECT * \n",
    "        FROM {table_name} \n",
    "        WHERE updated_at > '{max_updated_at}' AND updated_at <= '{today}'\n",
    "    ) AS {table_name}\n",
    "    \"\"\"\n",
    "    df = (spark.read\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/oltp\")\n",
    "        .option(\"dbtable\", query)\n",
    "        .option(\"user\", \"postgres\")\n",
    "        .option(\"password\", \"postgres\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .option(\"partitionColumn\", \"id\")\n",
    "        .option(\"lowerBound\", lower)\n",
    "        .option(\"upperBound\", upper)\n",
    "        .option(\"numPartitions\", \"8\")\n",
    "        .option(\"partitionColumn\", \"updated_at\")\n",
    "        .option(\"lowerBound\", lower)\n",
    "        .option(\"upperBound\", upper)\n",
    "        .option(\"numPartitions\", \"8\")\n",
    "        .option(\"fetchsize\", \"10000\")\n",
    "        .load()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def ingest(\n",
    "    table_name: str, \n",
    "    df: DataFrame,\n",
    "    ts: date\n",
    ") -> bool:\n",
    "    # Thêm các cột cần thiết để theo dõi\n",
    "    batch_id = str(uuid.uuid4())\n",
    "    df_bronze = (df\n",
    "      .withColumn(\"_ingested_at\", F.lit(ts))\n",
    "      .withColumn(\"_batch_id\", F.lit(batch_id))\n",
    "      .withColumn(\"_is_deleted\", F.lit(False))\n",
    "    )\n",
    "    \n",
    "    # Ghi vào Iceberg\n",
    "    df_bronze.writeTo(f\"nessie.bronze.{table_name}\").append()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a8b5a2d-d8b1-45c3-9a14-fe8ac7ed3d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2009-12-13\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n",
      "max_updated_at: 1999-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower: 2009-12-13, upper: 2012-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "category = read(table_name=\"category\", today=today)\n",
    "customer = read(table_name=\"customer\", today=today)\n",
    "customer_location = read(table_name=\"customer_location\", today=today)\n",
    "customer_phone = read(table_name=\"customer_phone\", today=today)\n",
    "location = read(table_name=\"location\", today=today)\n",
    "phone_number = read(table_name=\"phone_number\", today=today)\n",
    "product_category = read(table_name=\"product_category\", today=today)\n",
    "review = read(table_name=\"review\", today=today)\n",
    "product = read(table_name=\"shadow_product\", today=today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc9311-a9d7-44df-beaf-faa7563bf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import (\n",
    "#     StructType, StructField,\n",
    "#     LongType, StringType, DateType, TimestampType, BooleanType\n",
    "# )\n",
    "\n",
    "# # Định nghĩa schema giống hệt bảng category\n",
    "# category_schema = StructType([\n",
    "#     StructField(\"id\", LongType(), True),\n",
    "#     StructField(\"category_name\", StringType(), True),\n",
    "#     StructField(\"created_at\", DateType(), True),\n",
    "#     StructField(\"updated_at\", DateType(), True)\n",
    "# ])\n",
    "\n",
    "# # Tạo DataFrame rỗng với schema trên\n",
    "# category = spark.createDataFrame([], schema=category_schema)\n",
    "\n",
    "# # Kiểm tra\n",
    "# category.printSchema()\n",
    "# category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd952415-1d3b-451a-8333-e193fc4250bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest(table_name=\"category\", df=category, ts=ts)\n",
    "ingest(table_name=\"customer\", df=customer, ts=ts)\n",
    "ingest(table_name=\"customer_location\", df=customer_location, ts=ts)\n",
    "ingest(table_name=\"location\", df=location, ts=ts)\n",
    "ingest(table_name=\"customer_phone\", df=customer_phone, ts=ts)\n",
    "ingest(table_name=\"product_category\", df=product_category, ts=ts)\n",
    "ingest(table_name=\"review\", df=review, ts=ts)\n",
    "ingest(table_name=\"shadow_product\", df=product, ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b400ca2-c751-4a95-812b-f0481bf5e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(f\"MERGE BRANCH {bronze_layer_branch_name} INTO {etl_processing_branch_name} IN nessie\")\n",
    "spark.sql(f\"DROP BRANCH {bronze_layer_branch_name} IN nessie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb66d0-e424-4a77-8c0f-2cdf3281521d",
   "metadata": {},
   "source": [
    "## Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2a8a0-202b-4711-985c-dbcb10285d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_layer_branch_name = f\"feat/silver-layer-{ts.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE BRANCH\n",
    "    IF NOT EXISTS {silver_layer_branch_name}\n",
    "    IN nessie\n",
    "    FROM {etl_processing_branch_name};\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"USE REFERENCE {silver_layer_branch_name} IN nessie;\")\n",
    "spark.sql(\"LIST REFERENCES IN nessie\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5711a-3118-4faf-92ee-aa51f5865a83",
   "metadata": {},
   "source": [
    "### Schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea4db9-61ba-403f-9eb4-dba5f29f4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iceberg có chức năng merge schema cho việc evolution rồi. Nếu đổi tên cột thì mới không được thôi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26438d-a5a3-4fa9-9fe8-91b0d9966418",
   "metadata": {},
   "source": [
    "### Schema enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912d456-214e-449f-9b41-c6c7b54e1623",
   "metadata": {},
   "source": [
    "#### Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd533f3e-5672-4f3e-ac26-9ea625b2b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Các này thì tuỳ trường hợp, cái này đã thực hiện ở Bronze Layer rồi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb279d7-7210-46ee-a54c-230b3f70c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đổi tên cột, chỉ select những cột cần thiết\n",
    "customer = customer.drop(\"login_username\", \"login_password\", \"updated_at\")\n",
    "customer = customer.withColumnRenamed(\"created_at\", \"signup_date\").withColumnRenamed(\"id\", \"customer_id\")\n",
    "\n",
    "location = location.drop(\"created_at\", \"updated_at\")\n",
    "location = location.withColumnRenamed(\"id\", \"location_id\")\n",
    "\n",
    "customer_location = customer_location.withColumnRenamed(\"created_at\", \"source_created_at\").withColumnRenamed(\"updated_at\", \"source_updated_at\").withColumnRenamed(\"id\", \"customer_location_id\")\n",
    "\n",
    "product = product.drop(\"id\")\n",
    "product = product.withColumnRenamed(\"created_at\", \"source_created_at\").withColumnRenamed(\"updated_at\", \"source_updated_at\")\n",
    "\n",
    "review = review.drop(\"created_at\")\n",
    "review = review.withColumnRenamed(\"updated_at\", \"modified_date\").withColumnRenamed(\"id\", \"review_id\")\n",
    "\n",
    "category = category.drop(\"created_at\", \"updated_at\")\n",
    "category = category.withColumnRenamed(\"id\", \"category_id\")\n",
    "\n",
    "product_category = product_category.drop(\"created_at\", \"updated_at\")\n",
    "product_category = product_category.withColumnRenamed(\"id\", \"product_category_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec91e25-f42e-4b71-88d1-19edbe5d2197",
   "metadata": {},
   "source": [
    "#### Data Type Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b4dff-4ebb-40ac-b994-f0a77f582ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Các này thì tuỳ trường hợp, cái này đã thực hiện ở Bronze Layer rồi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d3403-dd61-446d-90c8-91709019f057",
   "metadata": {},
   "source": [
    "### Handling of null and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1fd52-cf2c-4829-9e0e-3698e2424bb6",
   "metadata": {},
   "source": [
    "#### customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117ff30f-47a7-4695-a810-ac5cb649f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.fillna(\n",
    "    {\n",
    "        \"name\": \"Unknown\",\n",
    "        \"sex\": \"Other\",\n",
    "        \"mail\": \"unknown@gmail.com\",\n",
    "        \"birthdate\": \"1900-01-01\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6b07e-d60d-49a4-a2c7-e6c9e0c016e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {customer.count()}\")\n",
    "customer = customer.na.drop(subset=[\"customer_id\"], how=\"any\")\n",
    "print(f\"Num of records after drop na: {customer.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90594e5-8ef0-4e1d-b33c-bb8bd5e250d5",
   "metadata": {},
   "source": [
    "#### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26918-59c9-4b6f-a4d0-93d0a2e947d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {location.count()}\")\n",
    "location = location.na.drop()\n",
    "print(f\"Num of records after drop na: {location.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b714e-73c5-44ff-a19d-30555ea67098",
   "metadata": {},
   "source": [
    "#### customer_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34423e-81ad-45e7-a3aa-1e85ca66b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {customer_location.count()}\")\n",
    "customer_location = customer_location.na.drop()\n",
    "print(f\"Num of records after drop na: {customer_location.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64750807-dc69-4a76-86c8-fca8b1888ef4",
   "metadata": {},
   "source": [
    "#### product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be561f55-04e4-4b6e-a677-c345445719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {product.count()}\")\n",
    "product = product.na.drop()\n",
    "print(f\"Num of records after drop na: {product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81ae06-ff6c-4083-9caf-f596e319b80a",
   "metadata": {},
   "source": [
    "#### category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700822aa-9d48-4237-8357-61c2a049910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {category.count()}\")\n",
    "category = category.na.drop()\n",
    "print(f\"Num of records after drop na: {category.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33fe4d0-7924-429c-b5c5-df294ab597d5",
   "metadata": {},
   "source": [
    "#### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4dbe6-cc74-41c1-a6d3-d4914735d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null các giá trị không ảnh hưởng đến báo cáo\n",
    "review = review.fillna(\n",
    "    {\n",
    "        \"helpful_votes\": 0,\n",
    "        \"total_votes\": 0,\n",
    "        \"marketplace\": \"Unknown\",\n",
    "        \"verified_purchase\": \"N\",\n",
    "        \"review_headline\": \"\",\n",
    "        \"review_body\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87a4ff-caf9-45c0-acd5-4850e64b9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {review.count()}\")\n",
    "review = review.na.drop(subset=[\"review_id\", \"customer_id\", \"product_id\", \"star_rating\"], how=\"any\")\n",
    "print(f\"Num of records after drop na: {review.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310aacf3-3664-45c5-b095-9ccaad3ce009",
   "metadata": {},
   "source": [
    "#### product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde40087-25ee-4e31-94ad-e06a28b19d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before drop na: {product_category.count()}\")\n",
    "product_category = product_category.na.drop()\n",
    "print(f\"Num of records after drop na: {product_category.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b90234-7347-479c-984f-8acefb83aceb",
   "metadata": {},
   "source": [
    "### Deduplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed9dae-983c-48e8-9ddc-3500c457f56e",
   "metadata": {},
   "source": [
    "#### customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e1298-0db2-4093-b3f9-ed3975abd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {customer.count()}\")\n",
    "customer = customer.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {customer.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52213b6f-fcba-45d3-8b41-a48587ae6dfc",
   "metadata": {},
   "source": [
    "#### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd71f9f-9cce-472e-82c9-a81fb314c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {location.count()}\")\n",
    "location = location.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {location.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc33ee-695a-475f-ab52-f64f070e274a",
   "metadata": {},
   "source": [
    "#### customer_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c764c0c-4389-4e66-97e1-242ae171e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {customer_location.count()}\")\n",
    "customer_location = customer_location.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {customer_location.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaf51d-29fe-43fc-bf01-ca72bfc65efc",
   "metadata": {},
   "source": [
    "#### product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2e2d0-49fe-40ce-9cfd-45cbb86d28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {product.count()}\")\n",
    "product = product.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {product.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344684e-2b65-4fbc-9311-4f0beaaf67b3",
   "metadata": {},
   "source": [
    "#### category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee4fe3-c208-43fb-844f-f1dcda8119a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {category.count()}\")\n",
    "category = category.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {category.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518734a-316e-49a9-ba25-389e77fa70ff",
   "metadata": {},
   "source": [
    "#### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6ba1f-e8dd-4b98-9d13-97ea1cd3e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {review.count()}\")\n",
    "review = review.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {review.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcdfe9-f9a2-4a26-b688-eced92bd9058",
   "metadata": {},
   "source": [
    "#### product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba668bf-9d24-4f16-bf1e-c3aa1bb5edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num of records before deduplicate: {product_category.count()}\")\n",
    "product_category = product_category.drop_duplicates()\n",
    "print(f\"Num of records after deduplicate: {product_category.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b3c1b-4eb3-4a7e-b389-00611924c2fb",
   "metadata": {},
   "source": [
    "### Resolution of out-of-order and late-arriving data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88dbdb-c883-4f5c-8daf-d69d8f9c0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chưa tìm hiểu kỹ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cb1cd-31d5-4658-b618-f88221991941",
   "metadata": {},
   "source": [
    "### Data quality checks and enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a90e62-520b-4183-bdd0-480102706b5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Sửa invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ce5f4-8351-4a3d-ad57-e3d0d0a68bb1",
   "metadata": {},
   "source": [
    "#### Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1cad0-a43d-4143-a35e-6e005f820732",
   "metadata": {},
   "source": [
    "##### Loại bỏ khoảng trắng không cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c85eb-8af6-4319-b374-c7329d822331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "\n",
    "def clean_whitespace(df):\n",
    "    for field in df.schema.fields:\n",
    "        if field.dataType.typeName() == \"string\":\n",
    "            df = df.withColumn(field.name, trim(df[field.name]))\n",
    "            # print(f\"Đã xử lý cột {field.name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baf1fe-c9dd-48d4-8da1-534487f0a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = clean_whitespace(df=customer)\n",
    "location = clean_whitespace(df=location)\n",
    "customer_location = clean_whitespace(df=customer_location)\n",
    "product = clean_whitespace(df=product)\n",
    "category = clean_whitespace(df=category)\n",
    "review = clean_whitespace(df=review)\n",
    "product_category = clean_whitespace(df=product_category)\n",
    "category = clean_whitespace(df=category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a28c7c0-ff30-4af6-9ca3-0d7b80f5ccd8",
   "metadata": {},
   "source": [
    "##### Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469032c9-4a0e-462b-b776-2614b8d58ec8",
   "metadata": {},
   "source": [
    "###### customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b86fc-7bf3-4192-b3ba-9cddfe04a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hoá cột \"sex\" cho trường hợp tổng hợp từ nhiều nguồn.\n",
    "customer = customer.withColumn(\"sex\", lower(col(\"sex\")))\n",
    "customer = customer.withColumn(\n",
    "    \"sex\",\n",
    "    when(col(\"sex\").isin(\"1\", \"m\", \"male\"), \"1\")\n",
    "    .when(col(\"sex\").isin(\"2\", \"f\", \"female\"), \"2\")\n",
    "    .when(col(\"sex\").isin(\"3\", \"o\", \"other\"), \"3\")\n",
    "    .otherwise(\"3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8655eec-682f-4073-a88e-546c46bb6204",
   "metadata": {},
   "source": [
    "###### category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c59d2-d74e-42b0-afdd-1f7872219625",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_category_name = [\n",
    "    'Personal_Care_Appliances',\n",
    "    'Toys',\n",
    "    'Beauty',\n",
    "    'Video Games',\n",
    "    'Digital_Ebook_Purchase',\n",
    "    'Watches',\n",
    "    'Pet Products',\n",
    "    'Grocery',\n",
    "    'Other',\n",
    "    'Mobile_Apps',\n",
    "    'Office Products',\n",
    "    'Camera',\n",
    "    'Wireless',\n",
    "    'Apparel',\n",
    "    'Automotive',\n",
    "    'Outdoors',\n",
    "    'Major Appliances',\n",
    "    'Furniture',\n",
    "    'Tools',\n",
    "    'Books',\n",
    "    'Musical Instruments',\n",
    "    'Baby',\n",
    "    'Health & Personal Care',\n",
    "    'Sports',\n",
    "    'Electronics',\n",
    "    'Mobile_Electronics',\n",
    "    'Shoes'\n",
    "]\n",
    "\n",
    "category = category.withColumn(\n",
    "    colName=\"category_name\",\n",
    "    col=when(\n",
    "            condition=col(\"category_name\").isin(valid_category_name), \n",
    "            value=col(\"category_name\")\n",
    "        ).otherwise(value=\"Other\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6bacf7-a3db-4c30-a8d0-7a31ca90850b",
   "metadata": {},
   "source": [
    "###### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988c772-be99-48c3-b303-682514c4989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hoá cột \"star_rating\" cho trường hợp tổng hợp từ nhiều nguồn.\n",
    "\n",
    "review = review.withColumn(\"star_rating\", lower(col(\"star_rating\")))\n",
    "review = review.withColumn(\n",
    "    \"star_rating\",\n",
    "    when(col(\"star_rating\").isin(\"one\", \"1\", \"1 star\", \"1*\", \"*\"), \"1\")\n",
    "    .when(col(\"star_rating\").isin(\"two\", \"2\", \"2 stars\", \"2*\", \"**\"), \"2\")\n",
    "    .when(col(\"star_rating\").isin(\"three\", \"3\", \"3 stars\", \"3*\", \"***\"), \"3\")\n",
    "    .when(col(\"star_rating\").isin(\"four\", \"4\", \"4 stars\", \"4*\", \"****\"), \"4\")\n",
    "    .when(col(\"star_rating\").isin(\"five\", \"5\", \"5 stars\", \"5*\", \"*****\"), \"5\")\n",
    "    .otherwise(\"9\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e098e9-773c-46a9-99d6-48bd1612b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hoá cột \"verified_purchase\" cho trường hợp tổng hợp từ nhiều nguồn.\n",
    "\n",
    "review = review.withColumn(\"verified_purchase\", lower(col(\"verified_purchase\")))\n",
    "review = review.withColumn(\n",
    "    \"verified_purchase\",\n",
    "    when(col(\"verified_purchase\").isin(\"yes\", \"y\", \"true\", \"t\", \"1\", \"verified\", \"purchased\", \"confirmed\"), \"True\")\n",
    "    .when(col(\"verified_purchase\").isin(\"no\", \"n\", \"false\", \"f\", \"0\", \"not verified\", \"unverified\", \"not purchased\"), \"False\")\n",
    "    .otherwise(\"False\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73307f-f0ae-44bf-a6d9-9e05d52dfcf0",
   "metadata": {},
   "source": [
    "##### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef6dca-33a8-4834-8b5b-3eb85aec948f",
   "metadata": {},
   "source": [
    "###### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf4351-2622-4cde-a46f-fdb130d4f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loại bỏ những giá trị không xác định của cột star_rating\n",
    "valid_star_rating = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "review = review.filter(col(\"star_rating\").isin(valid_star_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8be8df-bf4c-460f-bfb3-7e31101ab817",
   "metadata": {},
   "source": [
    "#### Type Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5fbfb-2e0d-4ea2-945b-ad3a803b9105",
   "metadata": {},
   "source": [
    "##### review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7628da2-c01e-41c7-8097-27bdd2ca415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đổi kiểu dữ liệu \"star_rating\" thành int\n",
    "review = review.withColumn(\"star_rating\", col(\"star_rating\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5882782-2b1b-43d4-98f5-0af42d079bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đổi giá trị Y/N của cột \"verified_purchase\" thành boolean\n",
    "review = review.withColumn(\"verified_purchase\", col(\"verified_purchase\").cast(\"boolean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101da596-4f0a-4fc0-80d6-da75bcfcdb65",
   "metadata": {},
   "source": [
    "##### location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080d89e-65f3-4fde-9ca3-611275ae784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đối datatype cột zipcode int->str. Dùng lpad cho thành 6 số.\n",
    "location = location.withColumn(\"zipcode\", lpad(col(\"zipcode\").cast(\"string\"), 6, \"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafc207-aaa5-41b0-8179-da2d20bf8427",
   "metadata": {},
   "source": [
    "### Merge branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adb6f7-4f98-4758-93e4-f334d9d4b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"TRUNCATE TABLE nessie.silver.customer\")\n",
    "customer.writeTo(\"nessie.silver.customer\").append()\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.location\")\n",
    "location.writeTo(\"nessie.silver.location\").append()\n",
    "\n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.customer_location\")\n",
    "customer_location.writeTo(\"nessie.silver.customer_location\").append()\n",
    "          \n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.product\")\n",
    "product.writeTo(\"nessie.silver.product\").append()\n",
    "          \n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.category\")\n",
    "category.writeTo(\"nessie.silver.category\").append()\n",
    "          \n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.review\")\n",
    "review.writeTo(\"nessie.silver.review\").append()\n",
    "          \n",
    "spark.sql(\"TRUNCATE TABLE nessie.silver.product_category\")\n",
    "product_category.writeTo(\"nessie.silver.product_category\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca9ae3-b392-40b7-a11e-571bc48130e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(f\"MERGE BRANCH {silver_layer_branch_name} INTO {etl_processing_branch_name} IN nessie\")\n",
    "spark.sql(f\"DROP BRANCH {silver_layer_branch_name} IN nessie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d392fc2d-bea6-40c7-8609-64e1ac0f1e8a",
   "metadata": {},
   "source": [
    "## Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d895ce-0b8d-43ed-af48-2d8657cb8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_layer_branch_name = f\"feat/gold-layer-{ts.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE BRANCH\n",
    "    IF NOT EXISTS {gold_layer_branch_name}\n",
    "    IN nessie\n",
    "    FROM {etl_processing_branch_name};\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"USE REFERENCE {gold_layer_branch_name} IN nessie;\")\n",
    "spark.sql(\"LIST REFERENCES IN nessie\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e916faa-cc4b-47f2-b4bb-0e8fb3aba641",
   "metadata": {},
   "source": [
    "### Slowly Changing Dimension (SCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0250e-406c-4579-b493-5e1606b5739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surrogate_key(\n",
    "    source: DataFrame,\n",
    "    target: DataFrame,\n",
    "    sk_name: str,\n",
    "    sort_cols: List[str]\n",
    ") -> DataFrame:\n",
    "    # Get max surrogate key in source table\n",
    "    max_sk = target.select(max(sk_name)).collect()[0][0]\n",
    "    max_sk = max_sk if max_sk is not None else 0\n",
    "\n",
    "    # Generate sk in silver table\n",
    "    source = source \\\n",
    "        .withColumn(sk_name, row_number().over(Window().orderBy(*sort_cols)) + max_sk) \\\n",
    "        .withColumn(sk_name, col(sk_name).cast(\"long\"))\n",
    "    \n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87add0b9-1d29-4d66-b87b-8e4fc1f74191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_bk_sk(\n",
    "    bridge_table: DataFrame,\n",
    "    dim_table: DataFrame,\n",
    "    bk_name: str,\n",
    "    sk_name: str\n",
    ") -> DataFrame:\n",
    "    bridge_table = bridge_table.join(\n",
    "        other=dim_table.select(bk_name, sk_name),\n",
    "        on=bk_name,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    bridge_table = bridge_table.drop(bk_name)\n",
    "    return bridge_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647345b-d2da-415c-ad22-d99b84d63e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ở trường hợp SCD1 (nghĩa là dữ liệu mới sẽ ghi đè lên dữ liệu cũ) thì ở OLTP cũng ghi đè thôi chứ không có bật CDC lên\n",
    "# -> Do vậy thì mỗi lần incremental load dựa trên cột updated_at thì mỗi dim_ID sẽ chỉ xuất hiện 1 lần\n",
    "# -> Xài MERGE INTO vô tư mà không cần phải xử lý deduplicate\n",
    "# Còn nếu SCD1 nhưng ở OLTP lỡ bật CDC thì chỉ cần xử lý deduplicate bằng cách chỉ lấy dim_ID có updated_at lớn nhất\n",
    "# Lưu ý: schema của Dim này chỉ hơn cái schema ở silver mỗi cái key thôi\n",
    "def SCD1(\n",
    "    source: DataFrame,\n",
    "    source_name: str,\n",
    "    target: DataFrame,\n",
    "    target_name: str,\n",
    "    sk_name: str,\n",
    "    bk_name: str\n",
    "):\n",
    "    # Generate surrogate key base on max(surrogate_key) in source table\n",
    "    source = generate_surrogate_key(source=source, target=target, sk_name=sk_name, sort_cols=[bk_name])\n",
    "\n",
    "    # Tiến hành SCD1, match thì overwrite, miss match thì insert\n",
    "    merge_expr = textwrap.dedent(f\"\"\"\n",
    "        MERGE INTO nessie.gold.{target_name} g\n",
    "        USING {source_name} s\n",
    "        ON s.{bk_name} = g.{bk_name}\n",
    "        WHEN MATCHED THEN \n",
    "            UPDATE SET {\", \".join([f\"g.{column} = s.{column}\" for column in target.columns if column != sk_name])}\n",
    "        WHEN NOT MATCHED THEN \n",
    "            INSERT ({\", \".join(sorted(target.columns))})\n",
    "            VALUES ({\", \".join(f\"s.{column}\" for column in sorted(target.columns))})\n",
    "    \"\"\")\n",
    "    print(merge_expr)\n",
    "    \n",
    "    # Exec expr\n",
    "    source.createOrReplaceTempView(source_name)\n",
    "    spark.sql(merge_expr)\n",
    "    spark.catalog.dropTempView(source_name)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49575caf-1a33-4608-b86c-653c0d645ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd2(\n",
    "    source: DataFrame,\n",
    "    source_name: str,\n",
    "    target: DataFrame,\n",
    "    target_name: str,\n",
    "    bk_name: str,\n",
    "    sk_name: str,\n",
    ") -> bool:\n",
    "    # Generate surrogate key base on max(surrogate_key) in source table\n",
    "    source = generate_surrogate_key(source=source, target=target, sk_name=sk_name, sort_cols=[bk_name])\n",
    "\n",
    "    # Tạo df chỉ chứa phiên bản cũ nhất của ID trong source table và Cập nhật bản mới nhất ở dim thành bản cũ\n",
    "    w = Window.partitionBy(bk_name).orderBy(F.col(\"source_updated_at\").asc())\n",
    "    earliest_record_in_source = source.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "    earliest_record_in_source.createOrReplaceTempView(source_name)\n",
    "\n",
    "    expr = textwrap.dedent(f\"\"\"\n",
    "        MERGE INTO nessie.gold.{target_name} g\n",
    "        USING {source_name} s\n",
    "        ON s.{bk_name} = g.{bk_name}\n",
    "        WHEN MATCHED AND g.is_current = True THEN \n",
    "            UPDATE SET g.is_current = False, g.valid_to = s.source_updated_at\n",
    "    \"\"\")\n",
    "    df = spark.sql(expr)\n",
    "    spark.catalog.dropTempView(source_name)\n",
    "\n",
    "    # Insert toàn bộ source vào target\n",
    "    w = Window.partitionBy(bk_name).orderBy(F.col(\"source_updated_at\").desc())\n",
    "    source = source \\\n",
    "                    .withColumn(\"valid_from\", col(\"source_updated_at\")) \\\n",
    "                    .withColumn(\"valid_to\", F.lag(\"valid_from\").over(w)) \\\n",
    "                    .drop(\"source_created_at\", \"source_updated_at\") \\\n",
    "                    .withColumn(\"is_current\", when(col(\"valid_to\").isNull(), True).otherwise(False))\n",
    "    source.writeTo(f\"nessie.gold.{target_name}\").append()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58f78b-048a-43d8-8fd8-7f8c94b1f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def generate_dimDate(\n",
    "    start_date: datetime,\n",
    "    end_date: datetime\n",
    "):\n",
    "    start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Sinh cột ngày liên tục\n",
    "    df_date = (\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as full_date\n",
    "        \"\"\")\n",
    "        .withColumn(\"date_key\", F.date_format(\"full_date\", \"yyyyMMdd\").cast(\"long\"))\n",
    "        .withColumn(\"day\", F.dayofmonth(\"full_date\"))\n",
    "        .withColumn(\"month\", F.month(\"full_date\"))\n",
    "        .withColumn(\"month_name\", F.date_format(\"full_date\", \"MMMM\"))\n",
    "        .withColumn(\"quarter\", F.quarter(\"full_date\"))\n",
    "        .withColumn(\"quarter_name\", F.concat(F.lit(\"Q\"), F.quarter(\"full_date\")))\n",
    "        .withColumn(\"year\", F.year(\"full_date\"))\n",
    "        .withColumn(\"day_of_week\", ((F.dayofweek(\"full_date\")+5)%7)+1)\n",
    "        .withColumn(\"day_name\", F.date_format(\"full_date\", \"EEEE\"))\n",
    "        .withColumn(\"week_of_year\", F.weekofyear(\"full_date\"))\n",
    "        .withColumn(\"is_weekend\", F.col(\"day_of_week\").isin(1,7))\n",
    "        .withColumn(\"is_holiday\", F.lit(False))\n",
    "    )\n",
    "\n",
    "    df_date.writeTo(f\"nessie.gold.dim_date\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88f25b-3bb4-4e9b-9020-b5d1fe0bf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact(\n",
    "    source: DataFrame,\n",
    "    target: DataFrame,\n",
    "    source_name: str,\n",
    "    target_name: str,\n",
    "    bk_name: str,\n",
    "    sk_name: str,\n",
    "    has_cur_col: bool\n",
    "):\n",
    "    # Generate surrogate key base on max(surrogate_key) in source table\n",
    "    source = generate_surrogate_key(source=source, target=target, sk_name=sk_name, sort_cols=[bk_name])\n",
    "\n",
    "    # Chỉnh is_current = False nếu có\n",
    "    source.createOrReplaceTempView(source_name)\n",
    "    expr = textwrap.dedent(f\"\"\"\n",
    "        MERGE INTO nessie.gold.{target_name} g\n",
    "        USING {source_name} s\n",
    "        ON s.{bk_name} = g.{bk_name}\n",
    "        WHEN MATCHED AND g.is_current = True AND {has_cur_col} = True THEN \n",
    "            UPDATE SET g.is_current = False\n",
    "    \"\"\")\n",
    "    spark.sql(expr)\n",
    "    spark.catalog.dropTempView(source_name)\n",
    "    print(expr)\n",
    "    \n",
    "    ## Append\n",
    "    if \"is_current\" in target.columns:\n",
    "        source = source.withColumn(\"is_current\", F.lit(True))\n",
    "    source.writeTo(f\"nessie.gold.{target_name}\").append()\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de31121-f34e-49b3-941d-86c370d3c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_category\n",
    "category = spark.table(\"nessie.silver.category\")\n",
    "dim_category = spark.table(\"nessie.gold.dim_category\")\n",
    "SCD1(\n",
    "    source=category,\n",
    "    target=dim_category,\n",
    "    source_name=\"category\",\n",
    "    target_name=\"dim_category\",\n",
    "    bk_name=\"category_id\",\n",
    "    sk_name=\"category_key\"\n",
    ") \n",
    "\n",
    "# Dim_customer\n",
    "customer = spark.table(\"nessie.silver.customer\")\n",
    "dim_customer = spark.table(\"nessie.gold.dim_customer\")\n",
    "SCD1(\n",
    "    source=customer,\n",
    "    target=dim_customer,\n",
    "    source_name=\"customer\",\n",
    "    target_name=\"dim_customer\",\n",
    "    bk_name=\"customer_id\",\n",
    "    sk_name=\"customer_key\"\n",
    ")\n",
    "\n",
    "# Dim_location\n",
    "location = spark.table(\"nessie.silver.location\")\n",
    "dim_location = spark.table(\"nessie.gold.dim_location\")\n",
    "SCD1(\n",
    "    source=location,\n",
    "    target=dim_location,\n",
    "    source_name=\"location\",\n",
    "    target_name=\"dim_location\",\n",
    "    bk_name=\"location_id\",\n",
    "    sk_name=\"location_key\"\n",
    ")\n",
    "\n",
    "# Dim_product\n",
    "product = spark.table(\"nessie.silver.product\")\n",
    "dim_product = spark.table(\"nessie.gold.dim_product\")\n",
    "scd2(\n",
    "    source=product,\n",
    "    source_name=\"product\",\n",
    "    target=dim_product,\n",
    "    target_name=\"dim_product\",\n",
    "    bk_name=\"product_id\",\n",
    "    sk_name=\"product_key\"\n",
    ")\n",
    "\n",
    "# Bridge_product_category\n",
    "product_category = spark.table(\"nessie.silver.product_category\")\n",
    "dim_category = spark.table(\"nessie.gold.dim_category\")\n",
    "dim_product = spark.table(\"nessie.gold.dim_product\").filter(col(\"is_current\")==True)\n",
    "bridge_product_category = spark.table(\"nessie.gold.bridge_product_category\")\n",
    "\n",
    "product_category = switch_bk_sk(\n",
    "    bridge_table=product_category, \n",
    "    dim_table=dim_category, \n",
    "    bk_name=\"category_id\", \n",
    "    sk_name=\"category_key\"\n",
    ")\n",
    "product_category = switch_bk_sk(\n",
    "    bridge_table=product_category, \n",
    "    dim_table=dim_product, \n",
    "    bk_name=\"product_id\", \n",
    "    sk_name=\"product_key\"\n",
    ")\n",
    "SCD1(\n",
    "    source=product_category,\n",
    "    target=bridge_product_category,\n",
    "    source_name=\"product_category\",\n",
    "    target_name=\"bridge_product_category\",\n",
    "    bk_name=\"product_category_id\",\n",
    "    sk_name=\"product_category_key\"\n",
    ")\n",
    "\n",
    "# Bridge_customer_location\n",
    "customer_location = spark.table(\"nessie.silver.customer_location\")\n",
    "dim_customer = spark.table(\"nessie.gold.dim_customer\")\n",
    "dim_location = spark.table(\"nessie.gold.dim_location\")\n",
    "bridge_customer_location = spark.table(\"nessie.gold.bridge_customer_location\")\n",
    "customer_location = switch_bk_sk(\n",
    "    bridge_table=customer_location, \n",
    "    dim_table=dim_customer, \n",
    "    bk_name=\"customer_id\", \n",
    "    sk_name=\"customer_key\"\n",
    ")\n",
    "\n",
    "customer_location = switch_bk_sk(\n",
    "    bridge_table=customer_location, \n",
    "    dim_table=dim_location, \n",
    "    bk_name=\"location_id\", \n",
    "    sk_name=\"location_key\"\n",
    ")\n",
    "\n",
    "scd2(\n",
    "    source=customer_location,\n",
    "    target=bridge_customer_location,\n",
    "    source_name=\"customer_location\",\n",
    "    target_name=\"bridge_customer_location\",\n",
    "    bk_name=\"customer_location_id\",\n",
    "    sk_name=\"customer_location_key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a95a2-bc27-4cda-b1cf-1bc9c2356346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dim_date\n",
    "generate_dimDate(\n",
    "    start_date=datetime(2000, 1, 1),\n",
    "    end_date=datetime(2015, 1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9c326-b36c-4a10-9da5-db21d72b832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact_review\n",
    "review = spark.table(\"nessie.silver.review\")\n",
    "review = review.withColumnRenamed(\"modified_date\", \"full_date\")\n",
    "\n",
    "dim_customer = spark.table(\"nessie.gold.dim_customer\")\n",
    "dim_product = spark.table(\"nessie.gold.dim_product\").filter(col(\"is_current\")==True)\n",
    "dim_date = spark.table(\"nessie.gold.dim_date\")\n",
    "fact_review = spark.table(\"nessie.gold.fact_review\")\n",
    "\n",
    "review = switch_bk_sk(\n",
    "    bridge_table=review, \n",
    "    dim_table=dim_customer, \n",
    "    bk_name=\"customer_id\", \n",
    "    sk_name=\"customer_key\"\n",
    ")\n",
    "\n",
    "review = switch_bk_sk(\n",
    "    bridge_table=review, \n",
    "    dim_table=dim_product, \n",
    "    bk_name=\"product_id\", \n",
    "    sk_name=\"product_key\"\n",
    ")\n",
    "\n",
    "review = switch_bk_sk(\n",
    "    bridge_table=review, \n",
    "    dim_table=dim_date, \n",
    "    bk_name=\"full_date\", \n",
    "    sk_name=\"date_key\"\n",
    ")\n",
    "\n",
    "fact(\n",
    "    source=review,\n",
    "    target=fact_review,\n",
    "    source_name=\"review\",\n",
    "    target_name=\"fact_review\",\n",
    "    bk_name=\"review_id\",\n",
    "    sk_name=\"review_key\",\n",
    "    has_cur_col=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a743102-cd0c-4851-a4cc-ee9f260b989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(f\"MERGE BRANCH {gold_layer_branch_name} INTO {etl_processing_branch_name} IN nessie\")\n",
    "spark.sql(f\"DROP BRANCH {gold_layer_branch_name} IN nessie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7364e6fc-0837-41bf-8527-9d6aa7c8cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE REFERENCE main IN nessie\")\n",
    "spark.sql(f\"MERGE BRANCH {etl_processing_branch_name} INTO main IN nessie\")\n",
    "spark.sql(f\"DROP BRANCH {etl_processing_branch_name} IN nessie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4234a-4111-4919-8159-3b803aea825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_category = spark.table(\"nessie.gold.dim_category\")\n",
    "# bridge_product_category = spark.table(\"nessie.gold.bridge_product_category\")\n",
    "# dim_product = spark.table(\"nessie.gold.dim_product\")\n",
    "# fact_review = spark.table(\"nessie.gold.fact_review\")\n",
    "# dim_customer = spark.table(\"nessie.gold.dim_customer\")\n",
    "# bridge_customer_location = spark.table(\"nessie.gold.bridge_customer_location\")\n",
    "# dim_location = spark.table(\"nessie.gold.dim_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc9a00-b9a6-4f9f-abf0-8a9b2cba9e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66960c05-94c4-4171-9cbb-2e0c6b8ccbd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Enrichment & Derived Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de337d03-7e09-4b2c-b441-9546da79b5c1",
   "metadata": {},
   "source": [
    "#### customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796edb3-97ef-4e20-a9cf-f3c9d1ebcecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b76c96-8342-403f-bb4d-bd359b9977c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### customer_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a24af0-4967-4876-a30a-9f7058df0a24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bd5d2-df9e-449c-a73b-2be5d044a40d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a6b0f-a4a2-4fe8-bf36-f533fd9ea6d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed7add-696e-474a-9c40-20464b369510",
   "metadata": {},
   "source": [
    "#### product_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f058d-7083-490f-b84b-15858b2bf297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
